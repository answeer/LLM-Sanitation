import httpx
from openai import OpenAI
from openai.types import Completion
import os
from dotenv import load_dotenv

def test_llm(model_name,test_query,endpoint,key):
    """Queries the LLM with a sample prompt."""
    print(f"\nQuerying model: {model_name}")
    print(f"Test query: {test_query}")
    try:
        client = OpenAI(
            http_client=httpx.Client(
                verify=False,
                trust_env=False
            ),
            api_key=key,
            base_url=endpoint
        )

        response: Completion = client.chat.completions.create(
            model=model_name,
            temperature=0.7,
            messages=[
                {
                    "role": "system",
                    "content": [
                        {
                            "type": "text",
                            "text": "You are an AI assistant that helps people find information."
                        }
                    ]
                },
                {
                    "role": "user",
                    "content": f"{test_query}"
                }
            ]
        )
        print(f"Response from {model_name}: {response.choices[0].message.content}")
    except Exception as e:
        print(f"Error querying LLM: {e}")


if __name__ == "__main__":
    load_dotenv()
    model_name = "claude-3-7-sonnet"
    test_query = "What is the capital of France?"
    endpoint = "https://************************t"
    key = "************"
    test_llm(model_name, test_query, endpoint, key)
